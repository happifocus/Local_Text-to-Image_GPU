{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377991f8-6a9e-4b62-a818-d93bfe5273bf",
   "metadata": {},
   "source": [
    "# Local Text-to-Image Generation using older Nvidia GPU and Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52da8ff-9e68-4642-83ea-0b523baa4413",
   "metadata": {},
   "source": [
    "- This notebook showcases the setup of a GPU-accelerated text-to-image generator.\n",
    "\n",
    "- To enhance the user experience, a Gradio interface is implemented for controlling the image generation process.\n",
    "\n",
    "- The text-to-image generator operates on a local GPU and a locally stored diffusion model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a607c-e150-4af6-aad5-f327c3236bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a6d47-d549-4966-a5d8-849a7182eccc",
   "metadata": {},
   "source": [
    "## 1. Create a Project Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676781a0-e7a6-44fb-a2a7-344b66bfc4b2",
   "metadata": {},
   "source": [
    "On your system, create a separate folder for the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462b7fa-3a83-43e2-bb3a-484c8fcccd30",
   "metadata": {},
   "source": [
    "## 2. Load the Safetensors Model File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad8af0-8358-4362-b7c1-f2f7727c0cb7",
   "metadata": {},
   "source": [
    "Download the safetensor model file: https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/blob/main/dreamlike-diffusion-1.0.safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7c612-1260-47fd-b002-0a6c5f3a581e",
   "metadata": {},
   "source": [
    "and move the file into the project folder along with the Jupyter notebook file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e1b97-06c5-4390-bb88-e87bcf5aa8e2",
   "metadata": {},
   "source": [
    "## 3. Anaconda Navigator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130916f-cd1b-4d9a-adee-e0ef79a0bf4f",
   "metadata": {},
   "source": [
    "**Environments**: Create a new environment, providing a name and selecting the Python package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21959190-bc6c-4f2a-a880-197086ce42c2",
   "metadata": {},
   "source": [
    "**Home**: Install JupyterLab and launch it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa413e9-77ad-4834-a33d-4f665edb43b0",
   "metadata": {},
   "source": [
    "## 4. JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9bf426-7831-4a85-9c9f-1f65a66ff757",
   "metadata": {},
   "source": [
    "Navigate to the project folder. Open the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd90879-7601-46b9-b71d-c137161941c7",
   "metadata": {},
   "source": [
    "Verify that you are working in the newly created environment by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918dd19-5c6c-43f1-8918-77b419c200d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a18b8-7f5d-4498-af7e-0b4823233839",
   "metadata": {},
   "source": [
    "## 5. Connecting CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fd92d-49e9-437f-be4f-9447f2460be5",
   "metadata": {},
   "source": [
    "In the Anaconda Navigator, go to the **Terminal** of the current environment and run: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "43f0f57b-f0e5-494d-8307-641b2073c13c",
   "metadata": {},
   "source": [
    "conda install pytorch torchvision pytorch-cuda -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84547e59-fbbe-436f-a6f2-1374d7771f43",
   "metadata": {},
   "source": [
    "## 6. Verifying CUDA-enabled GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b560f7a-6037-4ecb-a4c5-8ee5d565239d",
   "metadata": {},
   "source": [
    "Check if CUDA is available by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e350ef8-cc15-4288-bbc0-344d32b40ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26809e0-dcd3-4927-a149-040efcc8b7ee",
   "metadata": {},
   "source": [
    "## 7. Install Gradio, Diffusers, Transformers, Safetensors and OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248ab2e-4583-46b8-b03c-782ffb275fbb",
   "metadata": {},
   "source": [
    "Again, use the **Terminal** of the Anaconda environment for installation:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e48ad49-cd17-4a0f-a0ee-30c58816cc20",
   "metadata": {},
   "source": [
    "conda install conda-forge::gradio\n",
    "conda install diffusers\n",
    "conda install transformers\n",
    "pip install omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900aa6-364c-4491-85f6-b007ea827470",
   "metadata": {},
   "source": [
    "## 8. Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810f92f-0b63-4413-be4c-98bd3c6b9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler\n",
    "import safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad81f0-648d-4f7a-a760-665c14d8d8d8",
   "metadata": {},
   "source": [
    "## 9. Run the Code for Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f52ab1-1b12-42d3-ab13-bebb8d9d48c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_image(prompt, x_dimension, y_dimension, seed_enabled, seed, enable_inference_steps, num_inference_steps):\n",
    "   \n",
    "    # Check if all dimensions are provided\n",
    "    if prompt and x_dimension and y_dimension:\n",
    "        \n",
    "        # Initialize generator with a default value\n",
    "        generator = torch.Generator(\"cuda\")\n",
    "\n",
    "        # Define the seed for image generation if seed is enabled\n",
    "        if seed_enabled:\n",
    "            seed = int(seed)\n",
    "            generator.manual_seed(seed)\n",
    "        else:\n",
    "            # If seed is not enabled, reset the generator\n",
    "            generator.manual_seed(torch.seed())\n",
    "        \n",
    "        # Create diffusion pipeline, load safetensors model file from project folder (from subfolders: \"./\"dreamlike-diffusion-1.0.safetensors\")\n",
    "        pipe = StableDiffusionPipeline.from_single_file(\"dreamlike-diffusion-1.0.safetensors\", torch_dtype=torch.float32, use_safetensors=True)\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "\n",
    "        # Apply inference steps if enabled\n",
    "        if enable_inference_steps:\n",
    "            pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True) \n",
    "            image = pipe(prompt, generator=generator, num_inference_steps=num_inference_steps).images[0]\n",
    "        else:\n",
    "            image = pipe(prompt, generator=generator).images[0]\n",
    "\n",
    "        # Resize the image based on the provided dimensions\n",
    "        dimensions = (int(x_dimension), int(y_dimension))      \n",
    "        image = image.resize(dimensions)\n",
    "        \n",
    "        # After generating the image, clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Return generated and resized image\n",
    "        return image\n",
    "\n",
    "      \n",
    "# Options for the radio buttons\n",
    "dimensions = [\"512\", \"640\", \"768\", \"1024\"]\n",
    "\n",
    "# Build the Gradio interface\n",
    "iface_generate_image = gr.Interface(\n",
    "    fn=generate_image,\n",
    "    title=\"Text-to-Image Generator\", \n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Text Prompt\", placeholder=\"Type your prompt here...\"),\n",
    "        gr.Radio(label=\"X Dimension\", choices=dimensions),\n",
    "        gr.Radio(label=\"Y Dimension\", choices=dimensions),\n",
    "        gr.Checkbox(label=\"Enable Seed\"),\n",
    "        gr.Number(label=\"Set Seed Number\"),\n",
    "        gr.Checkbox(label=\"Enable Inference Steps\"),\n",
    "        gr.Slider(label=\"Set Inference Steps\"),\n",
    "    ],\n",
    "    outputs=gr.Image(label=\"Generated Image\")\n",
    "    #allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# start interface\n",
    "iface_generate_image.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c9d8e-5419-48d9-8a90-9f9203827cb9",
   "metadata": {},
   "source": [
    "**Use the following code to completely free up the GPU:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ed8b7-e117-430f-b70a-90b20315af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload GPU\n",
    "with torch.no_grad(): \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41aa26-55a4-4c4b-9af9-5a51200c1a8d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23230f0d-d9b0-4200-8a72-c72f63b31797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
